# -*- coding: utf-8 -*-
"""Cópia de CNN_Project_X_Ray.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sTlJQc8ml5Gvu67Vhnz6yr6KEUf0lKbE
"""

from google.colab import drive
from tensorflow.keras import layers
from tensorflow.keras import models,optimizers, Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import time
from datetime import timedelta
import matplotlib.pyplot as plt
import tensorflow as tf
drive.mount('/content/drive')
import matplotlib.pyplot as plt
#' ' means CPU whereas '/device:G:0' means GPU
import tensorflow as tf
tf.test.gpu_device_name()
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isn’t guaranteed
gpu = GPUs[0]
def printm():
 process = psutil.Process(os.getpid())
 print("Gen RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ), " | Proc size: " + humanize.naturalsize( process.memory_info().rss))
 print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()

"""Gráfico para acc e loss:"""

def plot_acc(history):
  plt.plot(range(1,len(history.history['accuracy'])+1),history.history['accuracy'],'bo', label='Training accuracy')
  plt.plot(range(1,len(history.history['accuracy'])+1),history.history['val_accuracy'],'b', label='Validation accuracy')
  plt.legend()
  plt.show()

def plot_loss(history):
  plt.plot(range(1,len(history.history['loss'])+1),history.history['loss'],'bo', label='Training loss')
  plt.plot(range(1,len(history.history['loss'])+1),history.history['val_loss'],'b', label='Validation loss')
  plt.legend()
  plt.show()

"""Image Generator Without Augmentation """

# rescale images 
train_data = ImageDataGenerator(rescale = 1/255) 
validation_data = ImageDataGenerator(rescale = 1/255)
                                                  
# Target_size(80,80)
train_generator_woa_80 =  train_data.flow_from_directory('/content/drive/My Drive/Deep learning/chest_xray/train',
                                                      target_size=(80,80),batch_size=16,class_mode='categorical',color_mode='grayscale')

validation_gen_woa_80= validation_data.flow_from_directory( '/content/drive/My Drive/Deep learning/chest_xray/val',
                                                      target_size=(80,80),batch_size=16,class_mode='categorical',color_mode='grayscale')

# Target_size(150,150)
train_generator_woa_150 =  train_data.flow_from_directory('/content/drive/My Drive/Deep learning/chest_xray/train',
                                                      target_size=(150,150),batch_size=16,class_mode='categorical',color_mode='grayscale')

validation_gen_woa_150= validation_data.flow_from_directory( '/content/drive/My Drive/Deep learning/chest_xray/val',
                                                      target_size=(150,150),batch_size=16,class_mode='categorical',color_mode='grayscale')

"""Model 1 - two convolution layers, (WITHOUT augmentation and WITHOUT dropout) and target size (150,150)"""

model_1 = models.Sequential()
model_1.add(layers.Conv2D(32,(3,3),activation = 'relu',input_shape=(150,150,1)))
model_1.add(layers.MaxPooling2D((2,2)))
model_1.add(layers.Conv2D(64,(3,3),activation = 'relu'))
model_1.add(layers.MaxPooling2D((2,2)))
model_1.add(layers.Flatten())
model_1.add(layers.Dense(128,activation='relu'))
model_1.add(layers.Dense(3,activation='softmax'))
model_1.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='rmsprop')

time_count = dict()
start_time = time.monotonic()
history1 = model_1.fit(train_generator_woa_150,steps_per_epoch=250,epochs=30, validation_data=validation_gen_woa_150, validation_steps=66) 
end_time = time.monotonic()
time_count['model_1'] = timedelta(seconds=end_time - start_time)

plot_loss(history1)

plot_acc(history1)

print(time_count['model_1'])

"""Model 2 - two convolution layers, (WITHOUT augmentation and WITH dropout 0.50) and  target size (150,150)"""

model_2 = models.Sequential()
model_2.add(layers.Conv2D(32,(3,3),activation = 'relu',input_shape=(150,150,1)))
model_2.add(layers.MaxPooling2D((2,2)))
model_2.add(layers.Conv2D(64,(3,3),activation = 'relu'))
model_2.add(layers.MaxPooling2D((2,2)))
model_2.add(layers.Flatten())
model_2.add(layers.Dropout(0.5))
model_2.add(layers.Dense(256,activation='relu'))
model_2.add(layers.Dense(3,activation='softmax'))

model_2.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='rmsprop')

start_time = time.monotonic()
history2 = model_2.fit(train_generator_woa_150,steps_per_epoch=250,epochs=30, validation_data=validation_gen_woa_150, validation_steps=66 ) 
end_time = time.monotonic()
time_count['model_2'] = timedelta(seconds=end_time - start_time)

plot_loss(history2)

plot_acc(history2)

print(time_count['model_2'])

"""Model 3 - two convolution layers, (WITH augmentation and WITH dropout 0.50) and  target size (150,150)

Image Generator with Augmentation, including geometrical transformation
"""

train_datagen_aug = ImageDataGenerator(
    rescale = 1./255,
    rotation_range = 40,
    width_shift_range=0.3,
    zoom_range = 0.2,
    horizontal_flip = True)

validation_datagen_aug = ImageDataGenerator(rescale = 1./255)

# Target_size(150,150)
train_generator_wa_150 =  train_datagen_aug.flow_from_directory('/content/drive/My Drive/Deep learning/chest_xray/train',
                                                      target_size=(150,150),batch_size=16,class_mode='categorical',color_mode='grayscale')

validation_generator_wa_150= validation_datagen_aug.flow_from_directory( '/content/drive/My Drive/Deep learning/chest_xray/val',
                                                      target_size=(150,150),batch_size=16,class_mode='categorical',color_mode='grayscale')

model_3 = models.Sequential()
model_3.add(layers.Conv2D(32, (3,3), activation = 'relu',
                                  input_shape = (150,150,1)))
model_3.add(layers.MaxPooling2D ((2,2)))
model_3.add(layers.Conv2D(64, (3,3), activation = 'relu'))
model_3.add(layers.MaxPooling2D ((2,2)))
model_3.add(layers.Flatten())
model_3.add(layers.Dropout(0.5))
model_3.add(layers.Dense(128, activation = 'relu'))
model_3.add(layers.Dense(3, activation = 'softmax'))

model_3.compile( optimizer = 'rmsprop',
                  loss='categorical_crossentropy',
                  metrics = ['accuracy'])

start_time = time.monotonic()
history3 = model_3.fit(train_generator_wa_150,steps_per_epoch=250,epochs=30, validation_data=validation_generator_wa_150, validation_steps=66 ) 
end_time = time.monotonic()
time_count['model_3'] = timedelta(seconds=end_time - start_time)

plot_loss(history3)

plot_acc(history3)

print(time_count['model_3'])

"""Model 4 - two convolution layers, (WITH augmentation and WITH dropout 0.50) and  target size (80,80)"""

# Target_size(80,80)
train_generator_wa_80 =  train_datagen_aug.flow_from_directory('/content/drive/My Drive/Deep learning/chest_xray/train',
                                                      target_size=(80,80),batch_size=16,class_mode='categorical',color_mode='grayscale')

validation_generator_wa_80= validation_datagen_aug.flow_from_directory( '/content/drive/My Drive/Deep learning/chest_xray/val',
                                                      target_size=(80,80),batch_size=16,class_mode='categorical',color_mode='grayscale')

model_4 = models.Sequential()
model_4.add(layers.Conv2D(32, (3,3), activation = 'relu',
                                  input_shape = (80,80,1)))
model_4.add(layers.MaxPooling2D ((2,2)))
model_4.add(layers.Conv2D(64, (3,3), activation = 'relu'))
model_4.add(layers.MaxPooling2D ((2,2)))
model_4.add(layers.Flatten())
model_4.add(layers.Dropout(0.5))
model_4.add(layers.Dense(128, activation = 'relu'))
model_4.add(layers.Dense(3, activation = 'softmax'))

model_4.compile( optimizer = 'rmsprop',
                  loss='categorical_crossentropy',
                  metrics = ['accuracy'])

start_time = time.monotonic()
history4 = model_4.fit(train_generator_wa_80,steps_per_epoch=250,epochs=30, validation_data=validation_generator_wa_80, validation_steps=66 ) 
end_time = time.monotonic()
time_count['model_4'] = timedelta(seconds=end_time - start_time)

plot_loss(history4)

plot_acc(history4)

print(time_count['model_4'])

"""Model 5 - three convolution layers, (WITH augmentation and WITH dropout 0.50) and  target size (80,80)"""

model_5= models.Sequential()
model_5.add(layers.Conv2D(32, (3, 3), activation = "relu", input_shape = (80, 80, 1)))
model_5.add(layers.MaxPooling2D((2,2)))
model_5.add(layers.Conv2D(64, (3, 3), activation = "relu"))
model_5.add(layers.MaxPooling2D((2,2)))
model_5.add(layers.Conv2D(128, (3, 3), activation = "relu"))
model_5.add(layers.MaxPooling2D((2,2)))
model_5.add(layers.Flatten())
model_5.add(layers.Dropout(0.5))
model_5.add(layers.Dense(256, activation = "relu"))
model_5.add(layers.Dense(3, activation = "softmax"))

model_5.compile(loss = "categorical_crossentropy", metrics = ["accuracy"], optimizer = "rmsprop")

start_time = time.monotonic()
history5 = model_5.fit(train_generator_wa_80,steps_per_epoch=250,epochs=30, validation_data=validation_generator_wa_80, validation_steps=66 ) 
end_time = time.monotonic()
time_count['model_5'] = timedelta(seconds=end_time - start_time)

plot_loss(history5)

plot_acc(history5)

print(time_count['model_5'])

"""Model 6 - Conv ->Conv ->max pooling->Conv ->Conv ->max pooling->dense layers(WITH augmentation and WITH drop out 0.50) and target size (80x80)"""

model_6 = models.Sequential()
model_6.add(layers.Conv2D(32,(3,3),activation = 'relu', input_shape = (80,80,1)))
model_6.add(layers.Conv2D(32,(3,3),activation = 'relu'))
model_6.add(layers.MaxPooling2D ((2,2)))
model_6.add(layers.Conv2D (64, (3,3), activation = 'relu'))
model_6.add(layers.Conv2D (64, (3,3), activation = 'relu'))
model_6.add(layers.MaxPooling2D ((2,2)))
model_6.add(layers.Flatten())
model_6.add(layers.Dropout(0.5))
model_6.add(layers.Dense(256,activation = 'relu'))
model_6.add(layers.Dense(3, activation = 'softmax'))


model_6.compile(optimizer = 'rmsprop',
                  loss='categorical_crossentropy',
                  metrics = ['accuracy'])

start_time = time.monotonic()
history6 = model_6.fit(train_generator_wa_80,steps_per_epoch=250,epochs=30, validation_data=validation_generator_wa_80, validation_steps=66 ) 
end_time = time.monotonic()
time_count['model_6'] = timedelta(seconds=end_time - start_time)

plot_loss(history6)

plot_acc(history6)

print(time_count['model_6'])

"""Model 7 - Conv ->Conv ->max pooling->Conv ->Conv ->max pooling->dense layers(WITH augmentation and WITH drop out 0.50), target size (80x80) and with RGB"""

# Target_size(80,80) with RGB
train_generator_wa_rgb_80 =  train_datagen_aug.flow_from_directory('/content/drive/My Drive/Deep learning/chest_xray/train',
                                                      target_size=(80,80),batch_size=16,class_mode='categorical',color_mode='rgb')

validation_generator_wa_rgb_80= validation_datagen_aug.flow_from_directory( '/content/drive/My Drive/Deep learning/chest_xray/val',
                                                      target_size=(80,80),batch_size=16,class_mode='categorical',color_mode='rgb')

model_7 = models.Sequential()
model_7.add(layers.Conv2D(32,(3,3),activation = 'relu', input_shape = (80,80,3)))
model_7.add(layers.Conv2D(32,(3,3),activation = 'relu'))
model_7.add(layers.MaxPooling2D ((2,2)))
model_7.add(layers.Conv2D (64, (3,3), activation = 'relu'))
model_7.add(layers.Conv2D (64, (3,3), activation = 'relu'))
model_7.add(layers.MaxPooling2D ((2,2)))
model_7.add(layers.Flatten())
model_7.add(layers.Dropout(0.5))
model_7.add(layers.Dense(256,activation = 'relu'))
model_7.add(layers.Dense(3, activation = 'softmax'))

model_7.compile(optimizer = 'rmsprop',
                  loss='categorical_crossentropy',
                  metrics = ['accuracy'])

start_time = time.monotonic()
history7 = model_7.fit(train_generator_wa_rgb_80,steps_per_epoch=250,epochs=30, validation_data=validation_generator_wa_rgb_80, validation_steps=66 ) 
end_time = time.monotonic()
time_count['model_7'] = timedelta(seconds=end_time - start_time)

plot_loss(history7)

plot_acc(history7)

print(time_count['model_7'])

"""Model 8 - Conv ->Conv ->max pooling->Conv ->Conv ->max pooling->dense layers(WITH augmentation but without geometrical transformation and WITH drop out 0.50), target size (80x80) and with RGB

Image Generator with Augmentation, WITHOUT geometrical transformation
"""

train_datagen_aug_nogt = ImageDataGenerator(
    rescale = 1./255,
    #rotation_range = 40, # Remove of Rotation Range
    width_shift_range=0.3,
    zoom_range = 0.2, 
    horizontal_flip = False) # Remove of Geometrical Transformation

validation_datagen_aug_nogt = ImageDataGenerator(rescale = 1./255)

# Target_size(80,80), without RGB 
train_generator_wa_nogt_80 =  train_datagen_aug_nogt.flow_from_directory('/content/drive/My Drive/Deep learning/chest_xray/train',
                                                      target_size=(80,80),batch_size=16,class_mode='categorical')

validation_generator_wa_nogt_80= validation_datagen_aug_nogt.flow_from_directory( '/content/drive/My Drive/Deep learning/chest_xray/val',
                                                      target_size=(80,80),batch_size=16,class_mode='categorical')

model_8 = models.Sequential()
model_8.add(layers.Conv2D(32,(3,3),activation = 'relu', input_shape = (80,80,3)))
model_8.add(layers.Conv2D(32,(3,3),activation = 'relu'))
model_8.add(layers.MaxPooling2D ((2,2)))
model_8.add(layers.Conv2D (64, (3,3), activation = 'relu'))
model_8.add(layers.Conv2D (64, (3,3), activation = 'relu'))
model_8.add(layers.MaxPooling2D ((2,2)))
model_8.add(layers.Flatten())
model_8.add(layers.Dropout(0.5))
model_8.add(layers.Dense(256,activation = 'relu'))
model_8.add(layers.Dense(3, activation = 'softmax'))

model_8.compile(optimizer = 'rmsprop',
                  loss='categorical_crossentropy',
                  metrics = ['accuracy'])

start_time = time.monotonic()
history8 = model_8.fit(train_generator_wa_nogt_80,steps_per_epoch=250,epochs=30, validation_data=validation_generator_wa_nogt_80, validation_steps=66 ) 
end_time = time.monotonic()
time_count['model_8'] = timedelta(seconds=end_time - start_time)

plot_loss(history8)

plot_acc(history8)

print(time_count['model_8'])

"""Model 9 - Conv ->Conv ->max pooling->Conv ->Conv ->max pooling->dense layers(WITH augmentation but without geometrical transformation and WITH drop out 0.20), target size (80x80) and with RGB"""

model_9 = models.Sequential()
model_9.add(layers.Conv2D(32,(3,3),activation = 'relu', input_shape = (80,80,3)))
model_9.add(layers.Conv2D(32,(3,3),activation = 'relu'))
model_9.add(layers.MaxPooling2D ((2,2)))
model_9.add(layers.Conv2D (64, (3,3), activation = 'relu'))
model_9.add(layers.Conv2D (64, (3,3), activation = 'relu'))
model_9.add(layers.MaxPooling2D ((2,2)))
model_9.add(layers.Flatten())
model_9.add(layers.Dropout(0.20))
model_9.add(layers.Dense(256,activation = 'relu'))
model_9.add(layers.Dense(3, activation = 'softmax'))

model_9.compile(optimizer = 'rmsprop',
                  loss='categorical_crossentropy',
                  metrics = ['accuracy'])

start_time = time.monotonic()
history9 = model_9.fit(train_generator_wa_nogt_80,steps_per_epoch=250,epochs=30, validation_data=validation_generator_wa_nogt_80, validation_steps=66 ) 
end_time = time.monotonic()
time_count['model_9'] = timedelta(seconds=end_time - start_time)

plot_loss(history9)

plot_acc(history9)

print(time_count['model_9'])

"""Model 10 - VGG16>dense layers(WITH augmentation but without geometrical transformation and WITH drop out 0.50), target size (80x80) and with RGB"""

from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing.image import img_to_array

# Target_size(80,80), without RGB 
train_generator_wa_nogt_rgb_80 =  train_datagen_aug_nogt.flow_from_directory('/content/drive/My Drive/Deep learning/chest_xray/train',
                                                      target_size=(80,80),batch_size=16,class_mode='categorical')

validation_generator_wa_nogt_rgb_80= validation_datagen_aug_nogt.flow_from_directory( '/content/drive/My Drive/Deep learning/chest_xray/val',
                                                      target_size=(80,80),batch_size=16,class_mode='categorical')

vgg16_model = VGG16(weights="imagenet", include_top=False, input_shape=(80, 80, 3))

for layer in vgg16_model.layers[:-4]:
    layer.trainable=False

model_10 = Sequential()
model_10.add(vgg16_model)
model_10.add(layers.Flatten())
model_10.add(layers.Dropout(0.2))
model_10.add(layers.Dense(256,activation='relu'))
model_10.add(layers.Dense(3,activation='softmax')) 

model_10.compile(optimizer = 'rmsprop',
                  loss='categorical_crossentropy',
                  metrics = ['accuracy'])

start_time = time.monotonic()
history10 = model_10.fit(train_generator_wa_nogt_rgb_80 ,steps_per_epoch=250,epochs=30, validation_data=validation_generator_wa_nogt_rgb_80, validation_steps=66 ) 
end_time = time.monotonic()
time_count['model_10'] = timedelta(seconds=end_time - start_time)

plot_loss(history10)

plot_acc(history10)

print(time_count['model_10'])

"""Check Accuracy Test Dataset  """

import tensorflow as tf
import os
def pred (test_path):
  y_pred= []
  for item in os.listdir(test_path):
      full_path = os.path.join(test_path, item)
      img = tf.keras.preprocessing.image.load_img(full_path, target_size=(80, 80),color_mode='rgb')
      img_tensor = tf.keras.preprocessing.image.img_to_array(img)  # (height, width, channels)
      img_tensor = np.expand_dims(img_tensor, axis=0) 
      img_tensor /= 255. 
      pred = model_9.predict(img_tensor)
      pred = pred>0.5
      y_pred.append(np.argmax(pred))

  return pd.Series(y_pred)

import pandas as pd
bac_path = '/content/drive/My Drive/Deep learning/chest_xray/test/BACTERIA'
bac_pred = pred(bac_path)
bac = pd.Series(np.full(len(bac_pred),0))
bac_total = pd.concat([bac,bac_pred],axis=1)

normal_path = '/content/drive/My Drive/Deep learning/chest_xray/test/NORMAL'
normal_pred = pred(normal_path)
normal = pd.Series(np.full(len(normal_pred),1))
normal_total = pd.concat([normal,normal],axis=1)

virus_path = '/content/drive/My Drive/Deep learning/chest_xray/test/VIRUS'
virus_pred = pred(virus_path)
virus = pd.Series(np.full(len(virus_pred),2))
virus_total = pd.concat([virus,virus_pred],axis=1)

df_label = pd.concat([bac_total,normal_total,virus_total]).rename(columns={0:'True_label',1:'Pred_label'})
confusion_matrix_df=df_label.groupby(by=['True_label','Pred_label']).size().unstack()
confusion_matrix_df['All']=confusion_matrix_df.apply(np.sum,axis=1)
confusion_matrix_df.loc['All'] = confusion_matrix_df.apply(np.sum,axis=0)
confusion_matrix_df
